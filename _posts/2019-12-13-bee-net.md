--- 
layout: post
title: Bee-Net A FCNN workflow 
---

My friend Jaylene has an apiary and as part of that is involved in scientific
projects sometimes.  Somehow she ended up with a bunch of images like this one:

{% include image.html url="/images/bee-net/example.jpg" description="Example
image to be processed. See the bees on the left?" %}

They scented a tupperware and counted the number of bees that visited each site;
scented or blank. The researchers were paying high school students $15 an hour
to count the bees in each image. This is pretty much a perfect use case of
machine learning; automating a fairly dull and easy task. 

So I decided to take a couple of stabs at the problem. This involves manually
segmenting images which I did with
[image-labeling-tool](https://bitbucket.org/ueacomputervision/image-labelling-tool/src/master/).
This is a python-based broswer tool with which you can draw arbitrary polygons
on an image. It works OK, and exports the coordinates of your polygons to JSON
which you then have to post-process. In the future I want to hand-roll a little
tkinter app that makes it really easy to segment images and save them to
single-channel PNGs. Anyways, this was the most time consuming part of the
process.

![_config.yml]({{ site.baseurl }}/images/bee-net/mask_and_image.jpg) |
![_config.yml]({{ site.baseurl }}/images/bee-net/mask_and_image.png)
:-------------------------:|:-------------------------: <em>Example
image...</em>             |  <em>and corresponding mask</em> 




## first approach: sliding window conv-net This was the state-of-the art in
image segmentation until
[FCNNs](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) came
onto the scene. The sliding window conv-net involves extracting little patches
from the image, usually of size around 32x32, and assigns each little patch with
a label: bee or not bee. Its advantage is that it allows for 1-to-1 class
balancing. For each positive example, you can feed to network a negative
example. However, for evaluating images is really slow, as you have to run the
conv-net forward on each pixel of the image to get a dense prediction. Since the
bee images are 1080x1920, this approach takes 10s of minutes for some patch
sizes, even with a GTX1070. Anyways, this method isn't really feasible when
trying to match human performance, as a large patch size is required to get good
accuracy. I have the patch-based code on github, but long ago I deleted it so it
would take a bit of searching to find. To actually count the bees, I had to
define an arbitrary threshold on the output of the neural network and draw
contours around that threshold. Counting the closed contours gave the number of
bees in the image, shown below. 

{% include image.html url="/images/bee-net/counted_011.png" description="Counted
bees. Red lines are contours of constant probability above some threshold." %}

This method gave pretty good results, with a miscount of +/-1 if I'm remembering
correctly. After I reached this stage in the project, I had to put it down
because of school and work until now. 


## second approach: Fully-Convolutional Neural Network (FCNN)

FCNNs are basically one-to-one mappings from input image to predicted
segmentation mask, compared to the sliding-window conv net which is a
many-to-one mapping from image patch to scalar prediction. Because they don't
rely on fully-connected layers, FCNNs can be applied to any image size. They
also don't do this extremely inefficient patch based sampling, so they are much
faster at producing predictions. However, I've run into a bunch of problems with
class imbalance for these neural networks. Typically bees comprise less than one
percent of the pixels in a given image, meaning negative examples contribute a
lot more to the overall loss. Anyways, I'll describe my current approach to
identifying bees in images, and how I'm working through this problem.






# method

code is all on [github](https://github.com/tcolligan4/bee-network).

It's probably worth noting that I found out about
[this](http://matpalm.com/blog/counting_bees/) after the first iteration of the
project. It's definitely worth a read and not so different than what I'm doing
here. 

The FCNN model architecture used is pretty simple: one downsampling layer with a
skip connection and a really small amount of filters. Anything more than this
and my MacBook wheezes. Google provides free GPUs on Colab, so I might
transition there once I feel like model complexity is the limiting factor.

{% include image.html url="/images/bee-net/bee-network.png" description="The
network used." %}

The images I work with are also 1080x1920, so I resized them to approximately
1/3 the dimension to ease training on my macbook.  I haven't implemented any
data augmentation, though I'm sure it would help. Another technique I can use is
using random crops of the image. One thing I don't think will help is random
zooming, as most bees stay around the same size in the image. The network is
training as I write this, so I'll update periodically.






