---
layout: post
published: true
updated: Thu Apr 9, 2020
---
Wed Apr 1 09:20:46 MDT 2020

- [x] Make sure data is properly registered. I visually inspected all of the path/rows over Montana and they all look perfectly registered.
- [x] Re-extract training data, removing all tiles where there are any 0 values (this corresponds to
  the edge of a path/row, and some LANDSAT tiles have different extent for the same path row.)
  Doing this seems to have increased validation accuracy and possibly f1 score. I didn't notice this
  issue until I manually examined all of the data by hand.

- [ ] See if I can download the missing data in over Montana, shown in the first figure [here]({%
  post_url 2020-03-31-2020-03-31-data-description %})

- [ ] Investigate why the 1m param network (described [here]({% post_url 2020-03-31-2020-03-31-model-comparison-table %})) did better than networks with way more parameters. Probably because the larger networks are way overparameterized and overfit.

- [ ] Train models with random start dates as described [here]({% post_url 2020-03-31-2020-03-31-data-description %}) 

- [x] Add more data: use all 7 bands available for surface reflectance data.

- [ ] Retrain recurrent networks on the same dataset as all the other networks.

- [x] Experiment with training networks with tiles taken over the centroids of irrigated polygons.
  Done. This definitely increases the performance of the networks on the test set.

- [ ] Weight orthogonality?

- [ ] Cycle-gan to pretrain the network. Pretrain the network on labels generated by another GAN (or VAE), to get realistic labels that aren't part of the training corpus.

- [ ] Train a network (to use as pretrained) to predict the mean image of its inputs. This may be
easier than training it to predict the next image.

{: style='list-style-type: none'}
