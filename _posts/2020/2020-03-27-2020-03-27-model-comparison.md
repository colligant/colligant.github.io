---
layout: post
published: true
---

---
# Fri Mar 27 08:40:19 MDT 2020: Research notes

Added multiprocessing to the data generator on Casper.

Running models with two different sets of data and the same model architecture.

Model architecture is a unet-type with only 3 downsampling layers and batch norm after each
convolution. Approximately 1M params.

One model is running on Casper and one on my computer. The Casper one is fitting surface reflectance
data over Montana for 2013 with images from may to october. There is also offline data augmentation
(no online augmentation) for the Casper model in the form of irrigated data being extracted over
centroids of each irrigated polygon as well as extraction through a raster scan.  My local one is
also using surf.  reflectance data over Montana from 2013, but the images are from March-April to
December (full year). There is no data augmentation (offline or online) for the model using
full-year data.

Learning rate schedule for full-year model:
Full year comprises of 13 stacked rgb images. 
```python
def lr_schedule(epoch):
    lr = 1e-3
    rlr = 1e-3
    if epoch > 50:
        rlr = lr / 2
    if epoch > 100:
        rlr = lr / 4
    if epoch > 150:
        rlr = lr / 6
    if epoch > 200:
        rlr = lr / 8
    if epoch > 250:
        rlr = lr / 16
    return rlr
```

For partial-year and offline data augmentation:
Partial year comprises of 9 stacked rgb images.

```python
def lr_schedule(epoch):
    lr = 1e-3
    rlr = 1e-3
    if epoch > 100:
        rlr = lr / 2
    if epoch > 200:
        rlr = lr / 4
    if epoch > 350:
        rlr = lr / 6
    if epoch > 400:
        rlr = lr / 8
    if epoch > 450:
        rlr = lr / 16
    if epoch > 500:
        rlr = lr / 32
    return rlr
```
---
# Sun Mar 29 10:03:41 MDT 2020: Cont.
The models mentioned above only reached ~.75 maximum f1 score on the validation set and
~.5 f1, respectively.
I also trained a unet model with ~5m parameters and the first lr schedule above on surface
reflectance data from the whole year. This reached ~.75 f1 as well.
Both models trained above for a few hundred epochs, ultimately getting to around 80-90% validation
accuracy, and overfitting a bit on the training set.

Currently, a model with 5m parameters is being trained on may-oct data that doesn't include offline
oversampling through centroid extraction.
If the current models don't do well, I'm going to try a few things:

- [] Make sure f1score metric is working as expected.
- [] Make sure data is clean.
- [] Deepen the UNet architecture to use four downsampling steps (like the original paper).
- [] Add more data: use all 7 bands available for surface reflectance data.
- [] Create table of model results.
